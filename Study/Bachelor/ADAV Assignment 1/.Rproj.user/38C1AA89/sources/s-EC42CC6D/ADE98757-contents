---
output:
  html_document: default
  pdf_document: default
---
## Assignment 1

waarom hebben we deze set gekozen??

For diffrent companies is having insight in shopping behaviour very interesting.
Predicting shopping behaviour could lead to better advertisment.

The inspected dataset in this analysis contains data about a young people survey from 2013, in Reykjavic (Slovak).   
The investigated relation is between hobbies and interests (x) and shopping behaviour (y). To take a closer look, different analyses approaches will be used.   
One limitation of this analysis is that the variables are measured on a 5 point Likert-scale. To investigate these types of variables, other approaches are more suitable than the ones used here. To work with this variables with the kind of analyses this assignment is about, we will treat these variables as continuous.  


## RQ: Which hobbies and/or interests influence shopping style?


### Library and data
```{r, message = FALSE}
library(tidyverse)
library(glmnet)
library(Metrics)
library(ggplot2)
library(reshape2)
data <- read.csv("responses.csv", header = T)
```

### Preparing data for analysis
The outcome variable is shopping behaviour. This variable is a compound of the questions about shopping behaviour.

This variable will be measured with the sum of scores of the 7 items about shopping behaviour. The first item does in contrast to the other 6 items measure in the opposite way. This is why we reversed the scale of the first item.

After this reverse, the sum of scores of the variable shopping behaviour is made. At last, the relevant columns were selected (hobbies and interests) in the subset with which we will continue the analysis. 
```{r}
#reverse scale so it matches the directions of the other items
data <- data %>%
      mutate(Finances_rev = 6 - Finances)

#sumscore and avg sumscore of shopping behaviour
data <- data %>%  
          mutate(shopping_beh = (Finances_rev + Shopping.centres + Branded.clothing + Entertainment.spending + Spending.on.looks + Spending.on.gadgets + Spending.on.healthy.eating))
  
#Select only the variables in the data that concern hobbies and interests and the shopping behaviour sumscore
data <- data %>%
  select(32:63, shopping_beh)
```

### Cleaning data
Unfortunately, working with matrixes does require the data to be complete (without NA's).
Deleting NA's after subsetting (previous step), will result in less dropped NA's, as rows with unrelated NA's are ignored.
After dropping the NA's, we have 877 observations left. 
```{r}
#how many rows contain NA's
sum(is.na(data))

#save the original data in case we need it later        nog iets mee doen
data_orignal <- data

#data without the NA's
data <- drop_na(data)
```

### Inspecting the data


The initial model does have a $R^2$ = .3471
F-statistic: 14.02 on 32 and 844 DF,  p-value: < 2.2e-16

```{r}
#full model Hobbies and Interests: y = shopping_beh  ~ all hobby and interests items 
fit_ols <- lm(shopping_beh ~ ., data = data) 
summary(fit_ols)

```

### Visualization of data 
```{r}
#for the first plot we lay out the hobbies and their prevalence to give an overview of the data
data_Mean <- colMeans(data)
data_Mean <- data_Mean[1:32]
names(data_Mean) <- str_replace_all(names(data_Mean), "[.]", " ")
data_names <- data.frame(names(data_Mean))
data_Mean <- data.frame(data_Mean)
names(data_Mean)[names(data_Mean) == "data_Mean"] <- "mean"
data_Mean[,2] <- data_names
names(data_Mean)[names(data_Mean) == "names.data_Mean."] <- "name"
data_Mean <- data_Mean[order(data_Mean$mean), ]
data_Mean$name <- factor(data_Mean$name, levels = data_Mean$name)

ggplot(data_Mean, aes(x=name, y=mean)) + 
  theme_bw() +
  geom_bar(stat="identity", width=.5, fill="#f8766d") + 
  labs(title="Ordered Bar Chart", 
       subtitle="Average Interest/Hobby Rating",
       x= 'Interest/Hobby name',
       y= "Average Rating") + 
  theme(axis.text.x = element_text(angle = 50, hjust = 1))





#For the second plot, we created a correlation matrix to look at the correlation between the different hobbies and interests and shopping behaviour. 
cormat <- cor(data[-33], data$shopping_beh)

#Reshape the data to make it suitable for the plot
melted_cormat <- melt(cormat)
melted_cormat$Var1<-str_replace_all(melted_cormat$Var1, "[.]", " ") # Convert names
melted_cormat$posneg <- ifelse(melted_cormat$value > 0, "apos", "neg")  # influence flag
melted_cormat <- melted_cormat[order(melted_cormat$value), ]  # sort
melted_cormat$Var1 <- factor(melted_cormat$Var1, levels = melted_cormat$Var1)  # convert to factor to retain sorted order in plot.

# plot2
ggplot(melted_cormat, aes(x=Var1, y=value, label=value)) + 
  geom_bar(stat='identity', aes(fill=posneg), width=0.6)  +
  scale_fill_manual(name="Type of effect", 
                    labels = c("Positive effect", "Negative effect"), 
                    values = c("apos"="#00ba38", "neg"="#f8766d")) + 
  labs(subtitle="Effect of hobbies on shopping behaviour", 
       title= "Diverging Bar Plot",
       x = 'Type of interest',
       y = 'Influence Value') + 
  coord_flip()


```


### Splitting data in a test and trainset
For using the test - train method, we divided the data in 70% (train) vs 30% (test)

```{r}
set.seed(123)

splits <- sample(c(rep("train", 614),
                rep("test", 263)))

data_master <- data %>% mutate(splits = splits)
data_train <- data_master %>% filter(splits == "train")
data_test <- data_master %>% filter(splits == "test")

```

## glmnet
We choose Lasso regression because this regression model does improve the readability of the outcome. Fitting the model with all the variables will often cause overfitting. Overfitting leads often to high train-set variance. To decrease this variance, the Lasso regression will bring different coefficient to slope = 0. This model does decrease variance by shrinking the coefficients and dropping irrelevant variables. 



### Deciding which value to choose for Lambda
Choosing the best value for lambda will be done with crossvalidation. 
The cvglm function gives two values for lambda. The first value is the best value for lambda (<lambda_min>).
To further shrink the coefficients, it is allowed to take the value which is 1 se from the best value.
This value for lambda will help to decrease the complexity of the model further. 


```{r}
#Creating matrix for lasso glmnet functions
x_train <- model.matrix(na.action = NULL, shopping_beh ~ ., data_train %>% select(-splits))
# The glmnet function for lasso regression will create a new intercept. The original intercept must be removed before using this regression model. 
x_train <- x_train[, -1]
head(x_train)

#for diffrent values of lambda
#This plot shows the increase of coefficients when the penalty does decrease
result_nolambda <- glmnet(x = x_train, y = data_train$shopping_beh, family = 'gaussian', alpha = 1)
plot(result_nolambda)

#cvglm() to choose a lambda value based on 5 folds
result_cv <- cv.glmnet(x = x_train, y = data_train$shopping_beh, nfolds = 5)
plot(result_cv)

#variables for the diffrent values of lambda, based on crossvalidation
lambda_1se <- result_cv$lambda.1se
lambda_min <- result_cv$lambda.min

#nonzero coefficients
result_cv

#Lasso regression models for two values for lambda (min & 1se)
lasso_min <- glmnet(x = x_train, y = data_train$shopping_beh, alpha = 1, lambda = lambda_min)
lasso_1se <- glmnet(x = x_train, y = data_train$shopping_beh, alpha = 1, lambda = lambda_1se)

#Shrinked model
coef(lasso_min)
coef(lasso_1se)

```

### Fitting lasso model on test set

```{r}
x_test <- model.matrix(shopping_beh ~ ., data = data_test %>% select(-splits))
x_test <- x_test[, -1]

y_pred_1se <- as.numeric(predict(lasso_1se, newx = x_test, s = lambda_1se))
y_pred_min <- as.numeric(predict(lasso_min, newx = x_test, s = lambda_min))

#tabble for predicted vs. observed
tibble(Predicted = y_pred_1se, Observed = data_test$shopping_beh) %>%
  ggplot(aes(x = Predicted, y = Observed)) +
  geom_point()+
  geom_abline(slope = 1, intercept = 0)+
  labs(title = "Predicted vs. observed")
```

### Conclusion

To decide which model will predict the most accurate, the MSE scores can be compared to each other. 

```{r}
#lasso 1se
mse_lasso_1se <- mse(data_test$shopping_beh, y_pred_1se)

#lasso min
mse_lasso_min <- mse(data_test$shopping_beh, y_pred_min)

#ols
data_train <- data_train[,-34] #delete column: splits
fit_ols_train <- lm(shopping_beh ~ ., data = data_train) 
y_pred_ols <- predict(fit_ols_train, newdata = data_test)
mse_ols <-mse(data_test$shopping_beh, y_pred_ols)

#comparing the models on 
mse_lasso_1se
mse_lasso_min
mse_ols
```


#plot Comparison of models for MSE

```{r}
#compiling result data
mse_data <- tibble(Method = as_factor(c("lasso", "lasso Min", "ols")), MSE_Value = c(mse_lasso_1se, mse_lasso_min, mse_ols))

ggplot(data = mse_data, aes(Method, MSE_Value, fill = Method)) +
  theme_classic() +
  labs(title="Result Bar Chart", 
       subtitle="Resulting values for the different MSE models",
       x= 'Method',
       y= "Score") +
  geom_bar(stat="identity", col = "black") + 
  coord_cartesian(ylim=c(16,18))
```


The result of the lasso regression model is unexpected. The lasso regression did not improve compared to the initial model (model with every predictor). The difference in MSE's between the models do not make any substantial prediction improvements. 
The results are not conform our expectations and we are not sure why this happened. 
A possible explanation is that our predictors, which are measured on a Likertscale, don't suit this kind of regression model.
This might have led to an unimproved value for the MSE.






